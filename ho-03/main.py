from unidecode import unidecode
import re
import nltk
from datetime import datetime
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.stem.porter import PorterStemmer
from nltk.stem.snowball import SnowballStemmer
import csv

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
"""
Functions
"""


def normalize_dates(text):
  # Regular expression pattern to match date strings in various formats
  date_pattern = r'(\d{1,2}[a-z]{2}\s+of\s+\d{4})|(\d{1,2}[\/\.\-]\d{1,2}[\/\.\-]\d{2,4})|(\d{1,2}[\/\.\-]\d{1,2}[\/\.\-]\d{4})'
  matches = re.findall(date_pattern, text)

  # Loop through all matches and normalize to the format DD/MM/YYYY
  for match in matches:
    original_date = ''.join(match)
    try:
      normalized_date = datetime.strptime(original_date,
                                          '%d%b %Y').strftime('%d/%m/%Y')
    except ValueError:
      try:
        normalized_date = datetime.strptime(original_date,
                                            '%d/%m/%Y').strftime('%d/%m/%Y')
      except ValueError:
        normalized_date = datetime.strptime(original_date,
                                            '%m/%d/%Y').strftime('%d/%m/%Y')
    text = text.replace(original_date, normalized_date)

  return text


def normalize_currency(text):
  # Regular expression pattern to match currency strings in various formats
  currency_pattern = r'\$\s*\d{1,3}(?:[,\.]\d{3})*(?:\.\d{2})?'
  matches = re.findall(currency_pattern, text)

  # Loop through all matches and normalize to the format $DD,DDD.DD
  for match in matches:
    original_currency = match
    amount = original_currency.replace('$', '').replace(',', '').strip()
    normalized_currency = '${:,.2f}'.format(float(amount))
    text = text.replace(original_currency, normalized_currency)

  return text


def remove_punctuation(text):
  # regular expression pattern to match all punctuation except in dates and currencies
  punctuation_pattern = r"[^\w\s\d\/.,$]|(?<!\d)\.|(?<!\d)\/|$(?<!\d)|(?<!\d)\,"

  cleaned_text = re.sub(punctuation_pattern, '', text)

  return cleaned_text


"""
Inputs
"""

#Test Text
test = "It's true, Ms. Martha TÃ¶pfer! $3.00 on 3/21/2023 in cash for an ice-cream in the U.S. market? :-( #Truth"

#Shakespeare file
input = open("../datasets/Shakespeare.txt")
shakespeare = input.read()
"""
Text Normalization
"""

testNormalized = unidecode(test)
testNormalized = testNormalized.lower()
testNormalized = normalize_dates(testNormalized)
testNormalized = normalize_currency(testNormalized)
testNormalized = remove_punctuation(testNormalized)

shakespeareNormalized = unidecode(shakespeare)
shakespeareNormalized = shakespeareNormalized.lower()
shakespeareNormalized = normalize_dates(shakespeareNormalized)
shakespeareNormalized = normalize_currency(shakespeareNormalized)
shakespeareNormalized = remove_punctuation(shakespeareNormalized)

print("\nTest String Normalized: " + testNormalized)
output = open("ShakespeareNormalized.txt", "w+")
output.write(shakespeareNormalized)
"""
Text Tokenization
"""
#White Space Tokenization

testToken01 = testNormalized.split()

shakespeareToken01 = shakespeareNormalized.split()

print("\nTest String 1: ", testToken01)
output = open("ShakespeareTokenized01.txt", "w+")
output.write("[" + (", ".join(shakespeareToken01)) + "]")
"""
Stop-words Removal
"""
testToken01_SW = [
  word for word in testToken01 if not word in stopwords.words()
]

sw = stopwords.words('english')

shakespeareToken01_SW = [word for word in shakespeareToken01 if not word in sw]

print("\nTest SW: ", testToken01_SW)
output = open("ShakespeareTokenized01_SW.txt", "w+")
output.write("[" + (", ".join(shakespeareToken01_SW)) + "]")
"""
Text Lemmatization
"""
lemmatizer = WordNetLemmatizer()

testToken01_00 = [lemmatizer.lemmatize(word) for word in testToken01_SW]
ShakespeareTokenized01_00 = [
  lemmatizer.lemmatize(word) for word in shakespeareToken01_SW
]

print("\nTest Lemmatized: ", testToken01_00)
output = open("ShakespeareTokenized01_00.txt", "w+")
output.write("[" + (", ".join(ShakespeareTokenized01_00)) + "]")
"""
Text Stemming
"""
# Porter Stemmer
porter = PorterStemmer()

testToken01_01 = [porter.stem(word) for word in testToken01_SW]
ShakespeareTokenized01_01 = [
  porter.stem(word) for word in shakespeareToken01_SW
]

print("\nTest Porter Stemmer: ", testToken01_01)
output = open("ShakespeareTokenized01_01.txt", "w+")
output.write("[" + (", ".join(ShakespeareTokenized01_01)) + "]")

# Snowball Stemmer
snow = SnowballStemmer("english")

testToken01_02 = [porter.stem(word) for word in testToken01_SW]
ShakespeareTokenized01_02 = [
  snow.stem(word) for word in shakespeareToken01_SW
]

print("\nTest Snowball Stemmer: ", testToken01_02)
output = open("ShakespeareTokenized01_02.txt", "w+")
output.write("[" + (", ".join(ShakespeareTokenized01_02)) + "]")
"""
Vocabulary Analysis
"""

# Data lists
lemmaData = []
porterData = []
snowData = []

# Count Tokens generated by Lemmatization
for token in ShakespeareTokenized01_00:
  found = False
  for row in lemmaData:
    if token in row:
      row[1] += 1
      found = True
  if (not found):
    lemmaData.append([token, 1, len(token)])

# Count Tokens generated by Porter Stemmer
for token in ShakespeareTokenized01_01:
  found = False
  for row in porterData:
    if token in row:
      row[1] += 1
      found = True
  if (not found):
    porterData.append([token, 1, len(token)])

# Count Tokens generated by Snowball Stemmer
for token in ShakespeareTokenized01_02:
  found = False
  for row in snowData:
    if token in row:
      row[1] += 1
      found = True
  if (not found):
    snowData.append([token, 1, len(token)])

# Write Files
lemmaFile = csv.writer(open("ShakespeareTokenized01_00.csv", "w+"))
lemmaFile.writerow(["Token", "Occurrences", "Size"])
for row in lemmaData:
  lemmaFile.writerow(row)

porterFile = csv.writer(open("ShakespeareTokenized01_01.csv", "w+"))
porterFile.writerow(["Token", "Occurrences", "Size"])
for row in porterData:
  porterFile.writerow(row)

snowFile = csv.writer(open("ShakespeareTokenized01_02.csv", "w+"))
snowFile.writerow(["Token", "Occurrences", "Size"])
for row in snowData:
  snowFile.writerow(row)

# Consolidated Data

lemmaTokensNumber = len(lemmaData)
porterTokensNumber = len(porterData)
snowTokensNumber = len(snowData)

lemmaAverageOccurrences = 0
porterAverageOccurrences = 0
snowAverageOccurrences = 0

lemmaAverageSize = 0
porterAverageSize = 0
snowAverageSize = 0

for row in lemmaData:
  lemmaAverageOccurrences += row[1]
  lemmaAverageSize += row[2]
lemmaAverageOccurrences /= lemmaTokensNumber
lemmaAverageSize /= lemmaTokensNumber

for row in porterData:
  porterAverageOccurrences += row[1]
  porterAverageSize += row[2]
porterAverageOccurrences /= porterTokensNumber
porterAverageSize /= porterTokensNumber

for row in snowData:
  snowAverageOccurrences += row[1]
  snowAverageSize += row[2]
snowAverageOccurrences /= snowTokensNumber
snowAverageSize /= snowTokensNumber

# Write File
output = open("ShakespeareTokenized01_VA.txt", "w+")
output.write('Lemmatization\n')
output.write(f'Vocabulary Size: {lemmaTokensNumber}\n')
output.write(f'Average Occurrences: {lemmaAverageOccurrences}\n')
output.write(f'Average Size: {lemmaAverageSize}\n')
output.write('\n')

output.write('Porter\n')
output.write(f'Vocabulary Size: {porterTokensNumber}\n')
output.write(f'Average Occurrences: {porterAverageOccurrences}\n')
output.write(f'Average Size: {porterAverageSize}\n')
output.write('\n')

output.write('Snowball\n')
output.write(f'Vocabulary Size: {snowTokensNumber}\n')
output.write(f'Average Occurrences: {snowAverageOccurrences}\n')
output.write(f'Average Size: {snowAverageSize}\n')
