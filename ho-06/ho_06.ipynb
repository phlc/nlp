{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouzU_0CHnuki",
        "outputId": "84d3d86d-b4f1-4813-bc15-3e4d536d7705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports and Auxiliary Functions for Vectorization"
      ],
      "metadata": {
        "id": "FfS_tALIpRK1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSqQTgNYgo7S"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from unidecode import unidecode\n",
        "from sklearn.datasets import fetch_20newsgroups as fetch_data\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "def remove_punctuation(text):\n",
        "  return re.sub(r\"[^\\w\\s\\d]\", '', text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data load"
      ],
      "metadata": {
        "id": "aujyqTkapnnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "\n",
        "for input in fetch_data().data:\n",
        "  input = unidecode(input)\n",
        "  input = input.lower()\n",
        "  input = remove_punctuation(input)\n",
        "  data.append(input)"
      ],
      "metadata": {
        "id": "0F6QXtiqoGP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF"
      ],
      "metadata": {
        "id": "XdQtsGWBp3zj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_idf_data = []\n",
        "tf_idf = TfidfVectorizer(smooth_idf=False)\n",
        "for input in data:\n",
        "    input_encoding = tf_idf.fit_transform(input.split('\\n')).toarray().tolist()\n",
        "    tf_idf_data.append(input_encoding)"
      ],
      "metadata": {
        "id": "sYWHd9NHp7V0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "tf_idf_data = np.array(tf_idf_data)"
      ],
      "metadata": {
        "id": "5wWFZZuIvr-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec"
      ],
      "metadata": {
        "id": "UmfPVhHerR4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = None\n",
        "for input in data:\n",
        "    words = []\n",
        "    for line in input.split('\\n'):\n",
        "      words.append(line.split())\n",
        "    model = Word2Vec(sentences=words,\n",
        "                     vector_size=100,\n",
        "                     window=5,\n",
        "                     min_count=1,\n",
        "                     workers=4)"
      ],
      "metadata": {
        "id": "IpHUSIjurUk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_data = model.wv.vectors"
      ],
      "metadata": {
        "id": "9BNQjCTyssyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Means (K=4)"
      ],
      "metadata": {
        "id": "uHJKR7LDvWY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans_tf_idf = KMeans(n_clusters=4, random_state=0, n_init=\"auto\").fit(tf_idf_data)\n",
        "kmeans_word2vec = KMeans(n_clusters=4, random_state=0, n_init=\"auto\").fit(word2vec_data)"
      ],
      "metadata": {
        "id": "ABMNJBWSvYDH"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spectral Clustering (K=6)"
      ],
      "metadata": {
        "id": "x4qJWjHwxkUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import SpectralClustering\n",
        "\n",
        "spectral_tf_idf = SpectralClustering(n_clusters=6, random_state=0).fit(tf_idf_data)\n",
        "spectral_word2vec = SpectralClustering(n_clusters=6, random_state=0).fit(word2vec_data)"
      ],
      "metadata": {
        "id": "VzgkzZKkxtnD"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gaussian Mixture"
      ],
      "metadata": {
        "id": "zHSWeKMXLWQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "gaussian_tf_idf = GaussianMixture(n_components=2, random_state=0).fit(tf_idf_data)\n",
        "gaussian_word2vec = GaussianMixture(n_components=2, random_state=0).fit(word2vec_data)"
      ],
      "metadata": {
        "id": "13VrsTRyLY2P"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agglomerative Clustering"
      ],
      "metadata": {
        "id": "7O5gE39nNDCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "agglomerative_tf_idf = AgglomerativeClustering().fit(tf_idf_data)\n",
        "agglomerative_word2vec = AgglomerativeClustering().fit(word2vec_data)"
      ],
      "metadata": {
        "id": "CK9njmgQNFNN"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DBSCAN"
      ],
      "metadata": {
        "id": "KWgpZ54RNbxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "dbscan_tf_idf = DBSCAN(eps=3, min_samples=2).fit(tf_idf_data)\n",
        "dbscan_word2vec = DBSCAN(eps=3, min_samples=2).fit(word2vec_data)"
      ],
      "metadata": {
        "id": "JY3KKJtmNdRk"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "HDBSCAN"
      ],
      "metadata": {
        "id": "IcFEFoJ-Nt-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hdbscan"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRSrcPSlOTRj",
        "outputId": "a5cbb796-c2f4-4c50-c050-ba6963c4268f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hdbscan\n",
            "  Downloading hdbscan-0.8.29.tar.gz (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from hdbscan) (1.22.4)\n",
            "Requirement already satisfied: cython>=0.27 in /usr/local/lib/python3.9/dist-packages (from hdbscan) (0.29.34)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.9/dist-packages (from hdbscan) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.9/dist-packages (from hdbscan) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.9/dist-packages (from hdbscan) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.20->hdbscan) (3.1.0)\n",
            "Building wheels for collected packages: hdbscan\n",
            "  Building wheel for hdbscan (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdbscan: filename=hdbscan-0.8.29-cp39-cp39-linux_x86_64.whl size=3580448 sha256=3e2da52bf9f149cc8833ab2b2d988c647f4687ebc650ce8ff1b4ee3501d49c7e\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/6f/88/1a4c04276b98306f00217a1e300e6ba0252c6aa4f7616067ae\n",
            "Successfully built hdbscan\n",
            "Installing collected packages: hdbscan\n",
            "Successfully installed hdbscan-0.8.29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import hdbscan\n",
        "\n",
        "hdbscan_tf_idf = hdbscan.HDBSCAN().fit(tf_idf_data)\n",
        "hbscan_word2vec = hdbscan.HDBSCAN().fit(word2vec_data)"
      ],
      "metadata": {
        "id": "PvuO3dK9NyH5"
      },
      "execution_count": 37,
      "outputs": []
    }
  ]
}