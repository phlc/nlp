# HO03: Lemmatization & Stemming
Sobre o texto `ShakespeareTokenized01.txt` resultante da Normalização e Tokenização (White Space Tokenization) realizada no HO02, realizar as seguintes ações em Python:

## Stop-words Removal
Realizar a remoção de stop-words do texto, e gerar um arquivo de saída denominado `ShakespeareTokenized01_SW.txt`:

## Text Lemmatization
Realizar a lematização do texto (já com stop-words removidas), utilizando o WordNet Lemmatizer e gerar um arquivo de saída denominado `ShakespeareTokenized01_00.txt`:

## Text Stemming
Aplicar cada um dos seguintes stemmers no arquivo de entrada `ShakespeareTokenized01_SW.txt` e gerar o arquivo de saída `ShakespeareTokenized01_XX.txt`, onde `XX` é o número da tarefa. Por exemplo, o arquivo `ShakespeareTokenized01_01.txt` é a saída do algoritmo 1 (Porter Stemmer):

1. Porter Stemmer
2. Snowball Stemmer

## Análise do Vocabulário
Comparar os vocabulários gerados por cada lematizador e stemmer utilizado, apresentando um arquivo `CSV` para cada um deles contendo:

1. Token (raíz resultante)
2. Número de ocorrências do token no documento resultante (lematizado ou com stemming)
3. Tamanho em caracteres de cada token do vocabulário

Por exemplo, para o lematizador, gerar o arquivo `ShakespeareTokenized01_00.csv` e para o Porter Stemmer gerar o arquivo `ShakespeareTokenized01_01.csv`.

Apresentar um documento final comparativo denominado `ShakespeareTokenized01_VA.txt` contendo, para cada lematizador e stemmer utilizado, o tamanho do vocabulário (número de tokens), o número médio de ocorrências e o tamanho médio dos tokens.

Disponibilizar o código-fonte em sua branch pessoal no repositório git dentro da pasta HO03.
